# Focal loss utilities for token classification
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional

class TokenFocalLoss(nn.Module):
    def __init__(self, gamma: float = 2.0, alpha: Optional[torch.Tensor] = None, ignore_index: int = -100, reduction: str = "mean"):
        super().__init__()
        self.gamma = gamma
        self.ignore_index = ignore_index
        self.reduction = reduction
        if alpha is not None and not isinstance(alpha, torch.Tensor):
            alpha = torch.tensor(alpha, dtype=torch.float)
        self.register_buffer("alpha", alpha if alpha is not None else None)

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # logits: (batch, seq_len, num_classes)
        # targets: (batch, seq_len)
        num_classes = logits.size(-1)
        logits_flat = logits.view(-1, num_classes)
        targets_flat = targets.view(-1)

        valid_mask = targets_flat != self.ignore_index
        if valid_mask.sum() == 0:
            return logits_flat.new_zeros(())

        logits_flat = logits_flat[valid_mask]
        targets_flat = targets_flat[valid_mask]

        log_probs = F.log_softmax(logits_flat, dim=-1)
        probs = log_probs.exp()

        # Gather log_probs for the true class
        true_log_probs = log_probs.gather(dim=-1, index=targets_flat.unsqueeze(1)).squeeze(1)
        true_probs = probs.gather(dim=-1, index=targets_flat.unsqueeze(1)).squeeze(1)

        focal_weight = (1.0 - true_probs).pow(self.gamma)
        if self.alpha is not None:
            alpha_weight = self.alpha.to(logits_flat.device).gather(0, targets_flat)
            focal_weight = focal_weight * alpha_weight

        loss = -focal_weight * true_log_probs

        if self.reduction == "mean":
            return loss.mean()
        if self.reduction == "sum":
            return loss.sum()
        return loss
import numpy as np
from tqdm import tqdm
from transformers import AutoTokenizer
import re
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback
import evaluate
from collections import Counter
import math

df= pd.read_csv("/content/drive/MyDrive/mapped_data (1).csv")
# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased", use_fast=True)

# Configuration
MAX_LENGTH = 512  # Model's max token limit
STRIDE = 128      # Overlap size
CHUNK_SIZE = MAX_LENGTH - 2  # Account for [CLS] and [SEP]

def find_dataset_positions(text, dataset_ids):
    """Find character positions of dataset mentions in text"""
    positions = []
    for ds_id in dataset_ids:
        # Handle different formatting of the same dataset ID
        patterns = [re.escape(ds_id)]
        if ds_id.startswith("https://doi.org/"):
            patterns.append(re.escape(ds_id.replace("https://doi.org/", "doi:")))
            patterns.append(re.escape(ds_id.split("/")[-1]))

        for pattern in patterns:
            for match in re.finditer(pattern, text):
                positions.append((match.start(), match.end(), "DATASET"))
    return positions

def create_chunks_with_labels(text, positions):
    """Process text into chunks with sliding window and create NER labels"""
    # Tokenize entire text to get accurate offsets
    encoding = tokenizer(text, return_offsets_mapping=True,
                         add_special_tokens=False, truncation=False)
    tokens = encoding["input_ids"]
    offset_mapping = encoding["offset_mapping"]

    # Create token-level labels (without special tokens)
    token_labels = ["O"] * len(tokens)
    for start_char, end_char, label_type in positions:
        entity_tokens = []
        for token_idx, (token_start, token_end) in enumerate(offset_mapping):
            # Check if token is inside entity span
            if (token_start >= start_char and token_end <= end_char) or \
               (start_char <= token_start < end_char) or \
               (start_char < token_end <= end_char):
                entity_tokens.append(token_idx)

        # Apply IOB tagging to entity tokens
        if entity_tokens:
            # Mark first token as B- and subsequent as I-
            token_labels[entity_tokens[0]] = f"B-{label_type}"
            for token_idx in entity_tokens[1:]:
                token_labels[token_idx] = f"I-{label_type}"

    # Create sliding window chunks
    chunks = []
    start = 0

    while start < len(tokens):
        end = min(start + CHUNK_SIZE, len(tokens))

        # Extract chunk tokens and labels
        chunk_tokens = tokens[start:end]
        chunk_labels = token_labels[start:end]

        # Add special tokens
        input_ids = [tokenizer.cls_token_id] + chunk_tokens + [tokenizer.sep_token_id]
        labels = [-100] + chunk_labels + [-100]  # -100 ignores special tokens in loss

        # Create attention mask
        attention_mask = [1] * len(input_ids)

        chunks.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
            "token_offset": start  # Track original position
        })

        # Exit if at end of text
        if end == len(tokens):
            break

        # Apply stride (with overlap)
        start += (CHUNK_SIZE - STRIDE)

    return chunks

# Group by article to process each paper
df = df
grouped = df.groupby("article_id").agg({
    "dataset_id": list,
    "text": "first"
})

# Process articles with sliding window chunking
ner_data = []
label_counter = Counter()

for article_id, row in tqdm(grouped.iterrows(), total=len(grouped)):
    text = row["text"]
    dataset_ids = row["dataset_id"]

    # Find dataset positions in text
    positions = find_dataset_positions(text, dataset_ids)

    # Create chunks with sliding window
    chunks = create_chunks_with_labels(text, positions)

    for chunk in chunks:
        # Collect label counts for weighting
        for label in chunk["labels"]:
            if label != -100:  # Ignore special tokens
                label_counter[label] += 1

        ner_data.append({
            "input_ids": chunk["input_ids"],
            "attention_mask": chunk["attention_mask"],
            "labels": chunk["labels"],
            "article_id": article_id
        })

# Convert to DataFrame
ner_df = pd.DataFrame(ner_data)
ner_df.to_pickle("ner_training_data.pkl")  # Save processed data


# Dataset class
class SciDataset(Dataset):
    def __init__(self, data, label2id):
        self.data = data
        self.label2id = label2id

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data.iloc[idx]

        # Convert string labels to IDs
        label_ids = []
        for tag in item["labels"]:
            if tag == -100:  # Special token
                label_ids.append(-100)
            else:
                label_ids.append(self.label2id.get(tag, -100))

        return {
            "input_ids": torch.tensor(item["input_ids"]),
            "attention_mask": torch.tensor(item["attention_mask"]),
            "labels": torch.tensor(label_ids)
        }

# Prepare label mapping
all_tags = set()
for labels in ner_df["labels"]:
    for tag in labels:
        if tag != -100:
            all_tags.add(tag)

label2id = {tag: i for i, tag in enumerate(sorted(all_tags))}
id2label = {i: tag for tag, i in label2id.items()}

# Calculate class weights
total_count = sum(label_counter.values())
class_weights = []

# Calculate weights with smoothing
for label in sorted(label2id.keys()):
    count = label_counter.get(label, 0)

    # Handle zero-count labels
    if count == 0:
        weight = 1.0
    else:
        # Inverse frequency weighting with sqrt smoothing
        weight = math.sqrt(total_count / (count + 1))  # +1 to avoid division by zero

    class_weights.append(weight)

# Convert to tensor
class_weights = torch.tensor(class_weights, dtype=torch.float)
# Normalize to mean=1.0 for focal alpha stability
alpha_weights = class_weights / (class_weights.mean() + 1e-8)
print("\nClass weights (raw):")
for label, weight in zip(sorted(label2id.keys()), class_weights):
    print(f"{label}: {weight:.2f}")

# Split data
train_df, val_df = train_test_split(ner_df, test_size=0.2, random_state=42)

train_dataset = SciDataset(train_df, label2id)
val_dataset = SciDataset(val_df, label2id)

# Data collator
data_collator = DataCollatorForTokenClassification(
    tokenizer,
    padding=True,
    max_length=MAX_LENGTH,
    label_pad_token_id=-100
)

# Initialize model
model = AutoModelForTokenClassification.from_pretrained(
    "allenai/scibert_scivocab_uncased",
    num_labels=len(label2id),
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True
)

# Custom Trainer with focal/weighted loss
class WeightedTrainer(Trainer):
    def __init__(self, class_weights, use_focal: bool = True, focal_gamma: float = 2.0, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.class_weights = class_weights.to(self.args.device)
        self.use_focal = use_focal
        self.focal_gamma = focal_gamma

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.logits

        weights = self.class_weights.to(logits.device)

        if self.use_focal:
            loss_fct = TokenFocalLoss(
                gamma=self.focal_gamma,
                alpha=weights,
                ignore_index=-100,
                reduction="mean",
            )
            loss = loss_fct(logits, labels)
        else:
            loss_fct = torch.nn.CrossEntropyLoss(
                weight=weights,
                ignore_index=-100,
            )
            loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))

        return (loss, outputs) if return_outputs else loss

# Metrics
seqeval = evaluate.load("seqeval")

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = seqeval.compute(predictions=true_predictions, references=true_labels)

    # Calculate F1 for DATASET classes separately
    dataset_metrics = {}
    for label in ["B-DATASET", "I-DATASET"]:
        if label in results:
            dataset_metrics.update({
                f"{label}_precision": results[label]["precision"],
                f"{label}_recall": results[label]["recall"],
                f"{label}_f1": results[label]["f1-score"],
                f"{label}_support": results[label]["support"],
            })

    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
        **dataset_metrics
    }

class GradualUnfreezeCallback(TrainerCallback):
    def __init__(self, last_n_layers: int = 3):
        self.last_n_layers = last_n_layers
        self._initialized = False

    def _get_base_and_layers(self, model):
        base = None
        for attr in ["bert", "roberta", "deberta", "deberta_v2", "electra", "distilbert", "longformer", "albert"]:
            if hasattr(model, attr):
                base = getattr(model, attr)
                break
        if base is None:
            return None, []
        encoder = getattr(base, "encoder", None) or getattr(base, "transformer", None)
        if encoder is None:
            return base, []
        layers = getattr(encoder, "layer", None) or getattr(encoder, "layers", None)
        if layers is None:
            return base, []
        return base, list(layers)

    def _freeze_all_except_classifier(self, model):
        for p in model.parameters():
            p.requires_grad = False
        if hasattr(model, "classifier"):
            for p in model.classifier.parameters():
                p.requires_grad = True

    def _unfreeze_last_k(self, model, k: int):
        base, layers = self._get_base_and_layers(model)
        if not layers:
            return
        k = max(0, min(k, len(layers)))
        for layer in layers[-k:]:
            for p in layer.parameters():
                p.requires_grad = True

    def on_train_begin(self, args, state, control, model=None, **kwargs):
        if not self._initialized:
            # Keep optimizer including all params; do not pre-freeze before optimizer creation
            # We still set requires_grad flags for clarity
            self._freeze_all_except_classifier(model)
            self._unfreeze_last_k(model, 1)
            self._initialized = True
        return control

    def on_epoch_begin(self, args, state, control, model=None, **kwargs):
        epoch_index = int(state.epoch) if state.epoch is not None else 0
        k = min(self.last_n_layers, epoch_index + 1)
        self._freeze_all_except_classifier(model)
        self._unfreeze_last_k(model, k)
        # Try to recreate optimizer/scheduler so newly unfrozen params are included
        if hasattr(self, "trainer") and getattr(self, "trainer") is not None:
            try:
                self.trainer.create_optimizer()
                self.trainer.create_scheduler(num_training_steps=self.trainer.args.max_steps)
            except Exception:
                pass
        return control

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=8,
    weight_decay=0.01,
    logging_dir="./logs",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    report_to="none",
    fp16=True,
    gradient_accumulation_steps=2,
    logging_steps=100,
    save_total_limit=2,
    warmup_ratio=0.1,
    max_grad_norm=1.0,
    group_by_length=True,
)

# Initialize Trainer with focal loss and callbacks
unfreeze_cb = GradualUnfreezeCallback(last_n_layers=3)
trainer = WeightedTrainer(
    class_weights=alpha_weights,
    use_focal=True,
    focal_gamma=2.0,
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2), unfreeze_cb],
)
# attach trainer reference to callback for optimizer recreation
unfreeze_cb.trainer = trainer

# Start training
trainer.train()
trainer.save_model("ner_model")
tokenizer.save_pretrained("ner_model")
